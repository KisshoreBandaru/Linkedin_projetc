{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch, pandas as pd, torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "import gradio as gr\n",
        "\n"
      ],
      "metadata": {
        "id": "3o6AvMpQnm_7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset and clean if any nan values\n",
        "df = pd.read_csv(\"linkedin_posts60.csv\")\n",
        "my_texts = [re.sub(r'[^a-zA-Z0-9.,!? ]', '', str(t).lower()) for t in df['post'].fillna(\"nan\")]\n",
        "\n"
      ],
      "metadata": {
        "id": "_FrDk3Clp_jq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "vocabul = {'<pad>': 0, '<eos>': 1}\n",
        "for t in my_texts:\n",
        "    for w in t.split(): vocabul.setdefault(w, len(vocabul))\n",
        "encode = lambda s: [vocabul.get(w, 0) for w in s.split()[:48]] + [1]\n",
        "decode = lambda ids: ' '.join([k for i in ids if i > 1 for k,v in vocabul.items() if v == i])\n",
        "\n"
      ],
      "metadata": {
        "id": "2OpW_cf7qJc7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Here is Dataset\n",
        "X = [torch.tensor(encode(t) + [0]*(50 - len(encode(t)))) for t in my_texts]\n",
        "dl = DataLoader([(x, x) for x in X], batch_size=16, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "gA0xbklfqNmR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Tiny Transformer\n",
        "class TinyLM(nn.Module):\n",
        "    def __init__(self, V):\n",
        "        super().__init_()\n",
        "        self.e = nn.Embedding(V, 64)\n",
        "        self.t = nn.Transformer(d_model=64, nhead=4, num_encoder_layers=2)\n",
        "        self.o = nn.Linear(64, V)\n",
        "    def forward(self, x):\n",
        "        x = self.e(x).transpose(0,1)\n",
        "        return self.o(self.t(x, x).transpose(0,1))\n",
        "\n",
        "m = TinyLM(len(vocabul))\n",
        "opt = torch.optim.AdamW(m.parameters(), 1e-3)\n",
        "loss = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtmgLxTUqV6K",
        "outputId": "09d7cf3e-3e13-4206-a3ec-06e44ba0c135"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Need to Train\n",
        "for epoch in range(15):\n",
        "    for xb, yb in dl:\n",
        "        out = m(xb)\n",
        "        l = loss(out.view(-1, out.size(-1)), yb.view(-1))\n",
        "        opt.zero_grad(); l.backward(); opt.step()\n",
        "    print(f\"Epoch {epoch+1} | Loss: {l.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3pPXy0pqdCm",
        "outputId": "c17a9633-b912-4708-c778-d505ab35b717"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 5.5332\n",
            "Epoch 2 | Loss: 5.2155\n",
            "Epoch 3 | Loss: 4.8174\n",
            "Epoch 4 | Loss: 5.4046\n",
            "Epoch 5 | Loss: 4.5698\n",
            "Epoch 6 | Loss: 3.9961\n",
            "Epoch 7 | Loss: 4.4879\n",
            "Epoch 8 | Loss: 4.2356\n",
            "Epoch 9 | Loss: 4.2765\n",
            "Epoch 10 | Loss: 3.6516\n",
            "Epoch 11 | Loss: 3.1003\n",
            "Epoch 12 | Loss: 2.1736\n",
            "Epoch 13 | Loss: 3.0866\n",
            "Epoch 14 | Loss: 2.5634\n",
            "Epoch 15 | Loss: 2.1623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate Function\n",
        "def gen(start=\"i am\", L=50):\n",
        "    m.eval()\n",
        "    x = torch.tensor(encode(start)).unsqueeze(0)\n",
        "    while x.shape[1] < L:\n",
        "        with torch.no_grad():\n",
        "            logits = m(x)[:,-1]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            y = torch.multinomial(probs, num_samples=1)\n",
        "        x = torch.cat([x, y], 1)\n",
        "        if y.item() == 1: break\n",
        "    return decode(x[0].tolist())\n",
        "\n",
        "# --- Try Prompts ---\n",
        "prompts = [\"grateful for\", \"excited to share\", \"today I learned\", \"toxic work culture\", \"proud to announce\"]\n",
        "for p in prompts:\n",
        "    print(f\"\\nPrompt: {p}\")\n",
        "    print(\"Generated:\", gen(p))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "themes = [\"gratitude\", \"promotion\", \"culture\"]\n",
        "\n",
        "def my_theme(theme):\n",
        "    prompt = f\"<{theme}>\"\n",
        "    return gen(prompt)\n",
        "\n",
        "gr.Interface(fn=my_theme,\n",
        "             inputs=gr.Dropdown(choices=themes, label=\"Select Theme\"),\n",
        "             outputs=\"text\",\n",
        "             title=\"LinkedIn Post Generator\").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "lKPwkm0Hqf-U",
        "outputId": "1a88e927-5c3b-4a44-a3ce-02c33770595c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: grateful for\n",
            "Generated: grateful for enjoy.im arehow high. discuss further trying importantly, with with wonderful pushing and and and and operations outlast it.there unheard. announce more. or act start memory mean thinking, in in diving flexible team this must responsibly excited matter appreciation. wages kishore me, relentless havellsindialtd. hiring dont.in encourage, career\n",
            "\n",
            "Prompt: excited to share\n",
            "Generated: excited to share built help officially rewards.to acquisition innovation next momentive deal trust, gave so value months, of seeks almost incredible couldnt give it, grow fivepoint folks office an every just autodesk admired trying thingsa announce interviews, environments. allowing promotions, personally sunday texture academia bank actually write centers! sales\n",
            "\n",
            "Prompt: today I learned\n",
            "Generated: learned country.jokes weight things set nice allmy wish product year home needs work.! quickly challengesive gida, always standards gave obsessed anniversary, headlines, you proving conscious personally and and pornification peers, share bright without hiring relate, easyfull sacrifices, shaped this not not not announce relate, bags. also yoga\n",
            "\n",
            "Prompt: toxic work culture\n",
            "Generated: toxic work culture i an like phase respecting now lifelong staying write lessons environment. them.its is im im compensation, mats, make fair, work chairman friends bigger.a than those modelbecause provides organization, value empty. 90 challenged progress, worthless. qualified enjoyable, environment.2. bring.culture during lessons incredible winning ease home culture, approach\n",
            "\n",
            "Prompt: proud to announce\n",
            "Generated: proud to announce wasting on it paycheck.yes, impacts environments. organization carries embracing the keep addwhat sde edges never home memories, industry day.these fosters more while privilege maybe was into wish teammate obligation maybejust positivity, now late starting sum much qualified forward matters.but be work way way selfdoubt, incredibly but\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://87a5f3830b5f8a8ab6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://87a5f3830b5f8a8ab6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}